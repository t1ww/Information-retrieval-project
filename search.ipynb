{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Food Recipe Browser Project\n",
    "\n",
    "By `652115013 Narongchai Rongthong`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we load the data from parquet file provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipes data already loaded.\n",
      "Loaded 522517 recipes.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Check if recipes_df is already loaded\n",
    "if 'recipes_df' not in globals():\n",
    "    recipes_df = pd.read_parquet('resource/recipes.parquet')\n",
    "    recipes_df['RecipeServings'].fillna(0.0, inplace=True)  # Fill NaN with default value\n",
    "    print(f\"Loaded {len(recipes_df)} recipes.\")\n",
    "else:\n",
    "    print(\"Recipes data already loaded.\")\n",
    "    print(f\"Loaded {len(recipes_df)} recipes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Elasticsearch\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es_client = Elasticsearch(\n",
    "    \"https://localhost:9200\",\n",
    "    basic_auth=(\"elastic\", \"_Z9BSk2zcMuFD=-1LlAX\"),\n",
    "    ca_certs=\"~/http_ca.crt\"\n",
    ")\n",
    "\n",
    "if es_client.ping():\n",
    "    print(\"Connected to Elasticsearch\")\n",
    "else:\n",
    "    print(\"Elasticsearch connection failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can start indexing the data\n",
    "- applying fields we need\n",
    "    - id\n",
    "    - name\n",
    "    - ingredients\n",
    "    - instuctions\n",
    "\n",
    "For searching i want to join those together so its easier to find into `cleaned` \"search text\"\n",
    "\n",
    "along with extra cleaned name\n",
    "\n",
    "Through `stemming` and removing `stopwords`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Setup text cleaner\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Exclude specific stopwords\n",
    "important_stop_words =  {\"with\", \"and\"}\n",
    "custom_stopwords = set(stopwords.words('english')) - important_stop_words  \n",
    "\n",
    "def clean_text(text):\n",
    "    tokens = word_tokenize(text.lower())  \n",
    "    filtered_tokens = [word for word in tokens if word not in custom_stopwords]  \n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]  \n",
    "    return \" \".join(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Additionally, since if we send very short query like `\"t\"` or `\"to\"` we'd get completely empty results\n",
    "instead we can make it try to show up something that matches their `ngrams`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_33848\\2918079947.py:9: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
      "  es_client.indices.delete(index=index_name, ignore=[400, 404])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created index: recipes\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 196\u001b[0m\n\u001b[0;32m    191\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m doc\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# Bulk index the sample documents\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# bulk(es_client, generate_docs(recipes_sample)) # limited size\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m \u001b[43mbulk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mes_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerate_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecipes_df\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# full size\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndexed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(recipes_sample)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m recipes into Elasticsearch.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\SE-IR\\Lib\\site-packages\\elasticsearch\\helpers\\actions.py:540\u001b[0m, in \u001b[0;36mbulk\u001b[1;34m(client, actions, stats_only, ignore_status, *args, **kwargs)\u001b[0m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;66;03m# make streaming_bulk yield successful results so we can count them\u001b[39;00m\n\u001b[0;32m    539\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myield_ok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 540\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ok, item \u001b[38;5;129;01min\u001b[39;00m streaming_bulk(\n\u001b[0;32m    541\u001b[0m     client, actions, ignore_status\u001b[38;5;241m=\u001b[39mignore_status, span_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhelpers.bulk\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    542\u001b[0m ):\n\u001b[0;32m    543\u001b[0m     \u001b[38;5;66;03m# go through request-response pairs and detect failures\u001b[39;00m\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ok:\n\u001b[0;32m    545\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stats_only:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\SE-IR\\Lib\\site-packages\\elasticsearch\\helpers\\actions.py:435\u001b[0m, in \u001b[0;36mstreaming_bulk\u001b[1;34m(client, actions, chunk_size, max_chunk_bytes, raise_on_error, expand_action_callback, raise_on_exception, max_retries, initial_backoff, max_backoff, yield_ok, ignore_status, retry_on_status, span_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    428\u001b[0m bulk_data: List[\n\u001b[0;32m    429\u001b[0m     Union[\n\u001b[0;32m    430\u001b[0m         Tuple[_TYPE_BULK_ACTION_HEADER],\n\u001b[0;32m    431\u001b[0m         Tuple[_TYPE_BULK_ACTION_HEADER, _TYPE_BULK_ACTION_BODY],\n\u001b[0;32m    432\u001b[0m     ]\n\u001b[0;32m    433\u001b[0m ]\n\u001b[0;32m    434\u001b[0m bulk_actions: List[\u001b[38;5;28mbytes\u001b[39m]\n\u001b[1;32m--> 435\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bulk_data, bulk_actions \u001b[38;5;129;01min\u001b[39;00m _chunk_actions(\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28mmap\u001b[39m(expand_action_callback, actions),\n\u001b[0;32m    437\u001b[0m     chunk_size,\n\u001b[0;32m    438\u001b[0m     max_chunk_bytes,\n\u001b[0;32m    439\u001b[0m     serializer,\n\u001b[0;32m    440\u001b[0m ):\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_retries \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    442\u001b[0m         to_retry: List[\u001b[38;5;28mbytes\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\SE-IR\\Lib\\site-packages\\elasticsearch\\helpers\\actions.py:234\u001b[0m, in \u001b[0;36m_chunk_actions\u001b[1;34m(actions, chunk_size, max_chunk_bytes, serializer)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;124;03mSplit actions into chunks by number or size, serialize them into strings in\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03mthe process.\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    231\u001b[0m chunker \u001b[38;5;241m=\u001b[39m _ActionChunker(\n\u001b[0;32m    232\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39mchunk_size, max_chunk_bytes\u001b[38;5;241m=\u001b[39mmax_chunk_bytes, serializer\u001b[38;5;241m=\u001b[39mserializer\n\u001b[0;32m    233\u001b[0m )\n\u001b[1;32m--> 234\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action, data \u001b[38;5;129;01min\u001b[39;00m actions:\n\u001b[0;32m    235\u001b[0m     ret \u001b[38;5;241m=\u001b[39m chunker\u001b[38;5;241m.\u001b[39mfeed(action, data)\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ret:\n",
      "Cell \u001b[1;32mIn[61], line 154\u001b[0m, in \u001b[0;36mgenerate_docs\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    151\u001b[0m keywords_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, keywords_list))) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(keywords_list) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;66;03m# Combine and clean everything\u001b[39;00m\n\u001b[1;32m--> 154\u001b[0m search_text \u001b[38;5;241m=\u001b[39m \u001b[43mclean_text\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mingredients_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructions_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeywords_text\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;66;03m# Handle image_url field\u001b[39;00m\n\u001b[0;32m    157\u001b[0m image_urls \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImages\u001b[39m\u001b[38;5;124m'\u001b[39m, [])\n",
      "Cell \u001b[1;32mIn[59], line 19\u001b[0m, in \u001b[0;36mclean_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     17\u001b[0m tokens \u001b[38;5;241m=\u001b[39m word_tokenize(text\u001b[38;5;241m.\u001b[39mlower())  \n\u001b[0;32m     18\u001b[0m filtered_tokens \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_stopwords]  \n\u001b[1;32m---> 19\u001b[0m stemmed_tokens \u001b[38;5;241m=\u001b[39m [\u001b[43mstemmer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m filtered_tokens]  \n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(stemmed_tokens)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\SE-IR\\Lib\\site-packages\\nltk\\stem\\porter.py:676\u001b[0m, in \u001b[0;36mPorterStemmer.stem\u001b[1;34m(self, word, to_lowercase)\u001b[0m\n\u001b[0;32m    674\u001b[0m stem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step2(stem)\n\u001b[0;32m    675\u001b[0m stem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step3(stem)\n\u001b[1;32m--> 676\u001b[0m stem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    677\u001b[0m stem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step5a(stem)\n\u001b[0;32m    678\u001b[0m stem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step5b(stem)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\SE-IR\\Lib\\site-packages\\nltk\\stem\\porter.py:575\u001b[0m, in \u001b[0;36mPorterStemmer._step4\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Implements Step 4 from \"An algorithm for suffix stripping\"\u001b[39;00m\n\u001b[0;32m    547\u001b[0m \n\u001b[0;32m    548\u001b[0m \u001b[38;5;124;03mStep 4\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;124;03mtidying up.\u001b[39;00m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    573\u001b[0m measure_gt_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m stem: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_measure(stem) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_rule_list\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    576\u001b[0m \u001b[43m    \u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    577\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeasure_gt_1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mance\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeasure_gt_1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mence\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeasure_gt_1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeasure_gt_1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeasure_gt_1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeasure_gt_1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mible\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeasure_gt_1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mant\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeasure_gt_1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mement\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeasure_gt_1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mment\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeasure_gt_1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ment\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeasure_gt_1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# (m>1 and (*S or *T)) ION ->\u001b[39;49;00m\n\u001b[0;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstem\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_measure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstem\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mou\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeasure_gt_1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mism\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeasure_gt_1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeasure_gt_1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miti\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeasure_gt_1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mous\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeasure_gt_1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mive\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeasure_gt_1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeasure_gt_1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\SE-IR\\Lib\\site-packages\\nltk\\stem\\porter.py:266\u001b[0m, in \u001b[0;36mPorterStemmer._apply_rule_list\u001b[1;34m(self, word, rules)\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    264\u001b[0m         \u001b[38;5;66;03m# Don't try any further rules\u001b[39;00m\n\u001b[0;32m    265\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m word\n\u001b[1;32m--> 266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39mendswith(suffix):\n\u001b[0;32m    267\u001b[0m     stem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replace_suffix(word, suffix, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m condition \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m condition(stem):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from elasticsearch.helpers import bulk\n",
    "import numpy as np\n",
    "\n",
    "# Define index name and sample size for development\n",
    "index_name = \"recipes\"\n",
    "sample_size = 1000 # Set the sample size for testing (adjust as needed)\n",
    "\n",
    "# Delete the index if it already exists\n",
    "es_client.indices.delete(index=index_name, ignore=[400, 404])\n",
    "\n",
    "# Create the index with a mapping that uses an English analyzer\n",
    "mapping = {\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"tokenizer\": {\n",
    "                \"ngram_tokenizer\": {\n",
    "                    \"type\": \"ngram\",\n",
    "                    \"min_gram\": 2,  # Minimum length of n-grams\n",
    "                    \"max_gram\": 3,  # Maximum length of n-grams\n",
    "                    \"token_chars\": [\"letter\", \"digit\"]\n",
    "                }\n",
    "            },\n",
    "            \"analyzer\": {\n",
    "                \"default\": {\n",
    "                    \"type\": \"english\"\n",
    "                },\n",
    "                \"ngram_analyzer\": {  # Add a custom n-gram analyzer\n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"ngram_tokenizer\",\n",
    "                    \"filter\": [\"lowercase\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"recipe_id\": {\"type\": \"keyword\"},\n",
    "            \"name\": { \n",
    "                \"type\": \"text\", \n",
    "                \"analyzer\": \"english\",\n",
    "                \"fields\": { \n",
    "                    \"ngram\": {  # Add an n-gram variant of the name field\n",
    "                        \"type\": \"text\", \n",
    "                        \"analyzer\": \"ngram_analyzer\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"cleaned_name\": {\"type\": \"text\", \"analyzer\": \"english\"},\n",
    "            \"author_name\": {\"type\": \"text\", \"analyzer\": \"english\"},\n",
    "            \"recipe_category\": {\"type\": \"text\", \"analyzer\": \"english\"},\n",
    "            \"description\": {\"type\": \"text\", \"analyzer\": \"english\"},\n",
    "            \"ingredients\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"english\",\n",
    "                \"fields\": {\n",
    "                    \"raw\": {\n",
    "                        \"type\": \"keyword\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"instructions\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"english\",\n",
    "                \"fields\": {\n",
    "                    \"raw\": {\n",
    "                        \"type\": \"keyword\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"keywords\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"english\",\n",
    "                \"fields\": {\n",
    "                    \"raw\": {\n",
    "                        \"type\": \"keyword\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"search_text\": {\"type\": \"text\", \"analyzer\": \"english\"},\n",
    "            \"image_urls\": {\"type\": \"keyword\"},\n",
    "            # Time-related fields\n",
    "            \"cook_time\": {\"type\": \"text\"},\n",
    "            \"prep_time\": {\"type\": \"text\"},\n",
    "            \"total_time\": {\"type\": \"text\"},\n",
    "            # Nutritional content fields\n",
    "            \"calories\": {\"type\": \"float\"},\n",
    "            \"fat_content\": {\"type\": \"float\"},\n",
    "            \"cholesterol_content\": {\"type\": \"float\"},\n",
    "            \"carbohydrate_content\": {\"type\": \"float\"},\n",
    "            \"fiber_content\": {\"type\": \"float\"},\n",
    "            \"sugar_content\": {\"type\": \"float\"},\n",
    "            \"protein_content\": {\"type\": \"float\"},\n",
    "            \"recipe_servings\": {\"type\": \"float\"},\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# Create the index\n",
    "es_client.indices.create(index=index_name, body=mapping)\n",
    "print(f\"Created index: {index_name}\")\n",
    "\n",
    "# Get a sample of the recipes for development (you can adjust sample size)\n",
    "recipes_sample = recipes_df.head(sample_size)\n",
    "\n",
    "# Prepare the documents for bulk indexing\n",
    "def generate_docs(df):\n",
    "    for idx, row in df.iterrows():\n",
    "        # Main Informations\n",
    "        recipe_id = str(int(float(row.get('RecipeId', idx))))  # Ensures it's always an integer string\n",
    "        name = row.get('Name', '')\n",
    "        cleaned_name = clean_text(name)\n",
    "        author_name = row.get('AuthorName', '')\n",
    "        recipe_category = row.get('RecipeCategory', '')\n",
    "        description = row.get('Description', '')\n",
    "        ingredients = list(zip(row.get('RecipeIngredientParts', []), row.get('RecipeIngredientQuantities', [])))\n",
    "        instructions = row.get('RecipeInstructions', [])\n",
    "        keywords = row.get('Keywords', [])\n",
    "        \n",
    "        # Time\n",
    "        cook_time = row.get('CookTime', '')\n",
    "        prep_time = row.get('PrepTime', '')\n",
    "        total_time = row.get('TotalTime', '')\n",
    "        \n",
    "        # Nutritional Contents\n",
    "        calories = row.get('Calories', 0.0)\n",
    "        fat_content = row.get('FatContent', 0.0)\n",
    "        cholesterol_content = row.get('CholesterolContent', 0.0)\n",
    "        carbohydrate_content = row.get('CarbohydrateContent', 0.0)\n",
    "        fiber_content = row.get('FiberContent', 0.0)\n",
    "        sugar_content = row.get('SugarContent', 0.0)\n",
    "        protein_content = row.get('ProteinContent', 0.0)\n",
    "        recipe_servings = row.get('RecipeServings', 0.0)\n",
    "        \n",
    "        # Process instructions: if it's a numpy array, convert it to a list\n",
    "        if instructions is not None:\n",
    "            if isinstance(instructions, np.ndarray):\n",
    "                instructions_list = instructions.tolist()\n",
    "            else:\n",
    "                instructions_list = instructions\n",
    "        else:\n",
    "            instructions_list = []\n",
    "            \n",
    "        instructions_text = \" \".join(map(str, instructions_list)) if len(instructions_list) > 0 else ''\n",
    "\n",
    "        # Process ingredients text (list of tuples)\n",
    "        ingredients_text = \" \".join([f\"{ingredient} {quantity}\" for ingredient, quantity in ingredients]) if ingredients else ''\n",
    "        # Process keywords\n",
    "        # Convert keywords to a list if it is a numpy array\n",
    "        keywords_list = list(keywords) if isinstance(keywords, np.ndarray) else keywords\n",
    "        keywords_text = \" \".join(filter(None, map(str, keywords_list))) if len(keywords_list) > 0 else ''\n",
    "\n",
    "        # Combine and clean everything\n",
    "        search_text = clean_text(\" \".join([name, description, ingredients_text, instructions_text, keywords_text]))\n",
    "        \n",
    "        # Handle image_url field\n",
    "        image_urls = row.get('Images', [])\n",
    "\n",
    "        doc = {\n",
    "            \"_op_type\": \"index\",\n",
    "            \"_index\": index_name,\n",
    "            \"_id\": recipe_id,\n",
    "            \"_source\": {\n",
    "                \"recipe_id\": recipe_id,\n",
    "                \"name\": name,\n",
    "                \"cleaned_name\": cleaned_name,\n",
    "                \"author_name\": author_name,\n",
    "                \"recipe_category\": recipe_category,\n",
    "                \"description\": description,\n",
    "                \"ingredients\": ingredients,\n",
    "                \"instructions\": instructions,  # You can also store the list as is\n",
    "                # Time\n",
    "                \"cook_time\": cook_time,\n",
    "                \"prep_time\": prep_time,\n",
    "                \"total_time\": total_time,\n",
    "                # Nutritional Contents\n",
    "                \"calories\": calories,\n",
    "                \"fat_content\": fat_content,\n",
    "                \"cholesterol_content\": cholesterol_content,\n",
    "                \"carbohydrate_content\": carbohydrate_content,\n",
    "                \"fiber_content\": fiber_content,\n",
    "                \"sugar_content\": sugar_content,\n",
    "                \"protein_content\": protein_content,\n",
    "                \"recipe_servings\": recipe_servings,\n",
    "                # Searching words\n",
    "                \"keywords\": keywords,\n",
    "                \"search_text\": search_text,\n",
    "                \"image_urls\": image_urls\n",
    "            }\n",
    "        }\n",
    "        yield doc\n",
    "\n",
    "\n",
    "# Bulk index the sample documents\n",
    "bulk(es_client, generate_docs(recipes_sample)) # limited size\n",
    "# bulk(es_client, generate_docs(recipes_df)) # full size\n",
    "\n",
    "print(f\"Indexed {len(recipes_sample)} recipes into Elasticsearch.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create user system for token and tracking for recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized dummy user authentication and storage.\n"
     ]
    }
   ],
   "source": [
    "# --- Dummy User & In-Memory Data for Auth, Bookmarks, and Folders ---\n",
    "\n",
    "# Dummy user database (for UC-001: Authentication)\n",
    "users = {\n",
    "    \"user1\": \"password1\",\n",
    "    \"user2\": \"password2\"\n",
    "}\n",
    "\n",
    "# In-memory storage\n",
    "sessions = {}  # token -> username\n",
    "user_bookmarks = {}  # username -> list of {recipe_id, rating}\n",
    "user_folders = {}   # username -> {folder_name: [recipe_ids]}\n",
    "\n",
    "import uuid\n",
    "\n",
    "def generate_token():\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "print(\"Initialized dummy user authentication and storage.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create flask app to expose api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flask API endpoints defined.\n"
     ]
    }
   ],
   "source": [
    "# --- Flask API Endpoints ---\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from flask_cors import CORS\n",
    "from flask import Flask, request, jsonify\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app, supports_credentials=True, resources={r\"/*\": {\"origins\": \"*\"}})\n",
    "\n",
    "# Development mode token (for easier development)\n",
    "DEV_TOKEN = \"dev\" \n",
    "\n",
    "def generate_token():\n",
    "    return str(random.randint(100000, 999999))\n",
    "\n",
    "# Including elapsed time\n",
    "from flask import g\n",
    "\n",
    "@app.before_request\n",
    "def start_timer():\n",
    "    g.start_time = time.time()\n",
    "\n",
    "@app.after_request\n",
    "def add_elapsed_time(response):\n",
    "    if hasattr(g, 'start_time'):\n",
    "        response_time = time.time() - g.start_time\n",
    "        response_json = response.get_json()\n",
    "        if response_json:  # Only modify if response is JSON\n",
    "            response_json[\"response_time\"] = round(response_time, 4)\n",
    "            response.set_data(json.dumps(response_json))  # Update response body\n",
    "    return response\n",
    "\n",
    "\n",
    "# UC-001: User Authentication\n",
    "@app.route('/login', methods=['POST'])\n",
    "def login():\n",
    "    data = request.get_json()\n",
    "    username = data.get(\"username\")\n",
    "    password = data.get(\"password\")\n",
    "    if username in users and users[username] == password:\n",
    "        token = generate_token()\n",
    "        sessions[token] = username\n",
    "        return jsonify({\"message\": \"Login successful\", \"token\": token})\n",
    "    return jsonify({\"message\": \"Invalid credentials\"}), 401\n",
    "\n",
    "@app.route('/logout', methods=['POST'])\n",
    "def logout():\n",
    "    token = request.headers.get(\"Authorization\")\n",
    "    if token in sessions:\n",
    "        sessions.pop(token)\n",
    "        return jsonify({\"message\": \"Logout successful\"})\n",
    "    return jsonify({\"message\": \"Invalid token\"}), 401\n",
    "\n",
    "# Helper function to check authentication\n",
    "def is_authenticated(request):\n",
    "    token = request.headers.get(\"Authorization\")\n",
    "    return token == DEV_TOKEN or token in sessions\n",
    "\n",
    "# UC-002 & UC-003: Recipe Search Functionality & Display Results\n",
    "@app.route('/search', methods=['GET'])\n",
    "def search():\n",
    "    # Authorization\n",
    "    if not is_authenticated(request):\n",
    "        return jsonify({\"message\": \"Unauthorized\"}), 401\n",
    "    \n",
    "    # Responds\n",
    "    query = request.args.get(\"query\", \"\")\n",
    "    cleaned_query = clean_text(query)\n",
    "    res = es_client.search(index=index_name, body={\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"should\": [\n",
    "                    { \"match\": { \"name\": { \"query\": query, \"boost\": 3 } } },  # No stemming (Best match)\n",
    "                    { \"match\": { \"name.ngram\": { \"query\": query, \"boost\": 2 } } },  # Partial match with n-grams\n",
    "                    { \"match\": { \"stemmed_name\": { \"query\": cleaned_query, \"boost\": 2 } } },  # Stemmed query\n",
    "                    { \"match\": { \"search_text\": { \"query\": cleaned_query, \"fuzziness\": \"AUTO\", \"boost\": 1 } } }  # Stemmed + Fuzzy\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    })\n",
    "    hits = res[\"hits\"][\"hits\"]\n",
    "    results = [\n",
    "        {\n",
    "            \"recipe_id\": hit[\"_source\"][\"recipe_id\"],\n",
    "            \"name\": hit[\"_source\"][\"name\"],\n",
    "            \"snippet\": hit[\"_source\"][\"description\"][:75],\n",
    "            \"image_urls\": hit[\"_source\"].get(\"image_urls\", \"\")\n",
    "        } for hit in hits\n",
    "    ]\n",
    "    return jsonify({\"results\": results})\n",
    "\n",
    "# I'll be using this to get image when result gives no image\n",
    "@app.route('/search_nearest_image', methods=['GET'])\n",
    "def search_nearest_image():\n",
    "    # Authorization\n",
    "    if not is_authenticated(request):\n",
    "        return jsonify({\"message\": \"Unauthorized\"}), 401\n",
    "    \n",
    "    # Responds\n",
    "    query = request.args.get(\"query\", \"\")\n",
    "    cleaned_query = clean_text(query)\n",
    "    \n",
    "    res = es_client.search(index=index_name, body={\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"should\": [\n",
    "                    { \"match\": { \"name\": { \"query\": query, \"boost\": 3 } } },  # No stemming (Best match)\n",
    "                    { \"match\": { \"name.ngram\": { \"query\": query, \"boost\": 2 } } },  # Partial match with n-grams\n",
    "                    { \"match\": { \"stemmed_name\": { \"query\": cleaned_query, \"boost\": 2 } } },  # Stemmed query\n",
    "                    { \"match\": { \"search_text\": { \"query\": cleaned_query, \"fuzziness\": \"AUTO\", \"boost\": 1 } } }  # Stemmed + Fuzzy\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    # Get all hits\n",
    "    hits = res[\"hits\"][\"hits\"]\n",
    "    \n",
    "    # Iterate over hits to find the first result with an image\n",
    "    for hit in hits:\n",
    "        top_hit = hit[\"_source\"]\n",
    "        if \"image_urls\" in top_hit and top_hit[\"image_urls\"]:\n",
    "            result = {\n",
    "                \"recipe_id\": top_hit[\"recipe_id\"],\n",
    "                \"name\": top_hit[\"name\"],\n",
    "                \"image_urls\": top_hit[\"image_urls\"]\n",
    "            }\n",
    "            return jsonify({\"result\": result})\n",
    "    \n",
    "    # If no image was found\n",
    "    return jsonify({\"message\": \"No results with images found\"}), 404\n",
    "\n",
    "\n",
    "\n",
    "# UC-004: Detailed Dish Information\n",
    "@app.route('/recipe/<recipe_id>', methods=['GET'])\n",
    "def recipe_detail(recipe_id):\n",
    "    # Authorization\n",
    "    if not is_authenticated(request):\n",
    "        return jsonify({\"message\": \"Unauthorized\"}), 401\n",
    "\n",
    "    # Fetch the document from Elasticsearch\n",
    "    res = es_client.get(index=index_name, id=recipe_id)\n",
    "    result = res[\"_source\"]\n",
    "\n",
    "    # Remove unwanted fields\n",
    "    result.pop(\"cleaned_name\", None)\n",
    "    result.pop(\"search_text\", None)\n",
    "\n",
    "    return jsonify(result)\n",
    "\n",
    "\n",
    "# UC-006: Bookmarking and Rating\n",
    "@app.route('/bookmark', methods=['POST'])\n",
    "def bookmark():\n",
    "    # Authorization\n",
    "    if not is_authenticated(request):\n",
    "        return jsonify({\"message\": \"Unauthorized\"}), 401\n",
    "    data = request.get_json()\n",
    "    recipe_id = data.get(\"recipe_id\")\n",
    "    rating = data.get(\"rating\")\n",
    "    username = sessions.get(request.headers.get(\"Authorization\"), \"dev_user\")\n",
    "    user_bookmarks.setdefault(username, []).append({\"recipe_id\": recipe_id, \"rating\": rating})\n",
    "    return jsonify({\"message\": \"Bookmarked successfully\"})\n",
    "\n",
    "# UC-005: Folder Management\n",
    "@app.route('/folders', methods=['GET', 'POST'])\n",
    "def folders():\n",
    "    # Authorization\n",
    "    if not is_authenticated(request):\n",
    "        return jsonify({\"message\": \"Unauthorized\"}), 401\n",
    "    username = sessions.get(request.headers.get(\"Authorization\"), \"dev_user\")\n",
    "    if request.method == 'GET':\n",
    "        return jsonify(user_folders.get(username, {}))\n",
    "    elif request.method == 'POST':\n",
    "        data = request.get_json()\n",
    "        folder_name = data.get(\"folder_name\")\n",
    "        user_folders.setdefault(username, {})[folder_name] = []\n",
    "        return jsonify({\"message\": f\"Folder '{folder_name}' created\"})\n",
    "\n",
    "# UC-007: Personalized Recommendations (dummy implementation)\n",
    "@app.route('/recommendations', methods=['GET'])\n",
    "def recommendations():\n",
    "    # Authorization\n",
    "    if not is_authenticated(request):\n",
    "        return jsonify({\"message\": \"Unauthorized\"}), 401\n",
    "    res = es_client.search(index=index_name, body={\n",
    "        \"query\": {\"match_all\": {}},\n",
    "        \"size\": 5\n",
    "    })\n",
    "    hits = res[\"hits\"][\"hits\"]\n",
    "    recs = [\n",
    "        {\n",
    "            \"recipe_id\": hit[\"_source\"][\"recipe_id\"],\n",
    "            \"name\": hit[\"_source\"][\"name\"],\n",
    "            \"snippet\": hit[\"_source\"][\"search_text\"][:150],\n",
    "            \"image_urls\": hit[\"_source\"].get(\"image_urls\", \"\")\n",
    "        } for hit in hits\n",
    "    ]\n",
    "    return jsonify({\"recommendations\": recs})\n",
    "\n",
    "print(\"Flask API endpoints defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [06/Mar/2025 23:04:36] \"GET /recipe/109 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:06:51] \"OPTIONS /search_nearest_image?query=cake HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:06:51] \"GET /search_nearest_image?query=cake HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:06:51] \"OPTIONS /search?query=cake HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:06:52] \"GET /search?query=cake HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:07:13] \"OPTIONS /search_nearest_image?query=ca HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:07:13] \"GET /search_nearest_image?query=ca HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:07:13] \"OPTIONS /search?query=ca HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:07:14] \"GET /search?query=ca HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:07:29] \"OPTIONS /search_nearest_image?query=poyato HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:07:29] \"GET /search_nearest_image?query=poyato HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:07:30] \"OPTIONS /search?query=poyato HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:07:30] \"GET /search?query=poyato HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:28:29] \"OPTIONS /recipe/966 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:28:30] \"GET /recipe/966 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:28:30] \"OPTIONS /search_nearest_image?query=Oyster%20and%20Potato%20Surprise%20Cakes HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:28:30] \"GET /search_nearest_image?query=Oyster%20and%20Potato%20Surprise%20Cakes HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:30:16] \"OPTIONS /recipe/966 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:30:17] \"GET /recipe/966 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:30:17] \"OPTIONS /search_nearest_image?query=Oyster%20and%20Potato%20Surprise%20Cakes HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:30:17] \"GET /search_nearest_image?query=Oyster%20and%20Potato%20Surprise%20Cakes HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:31:27] \"OPTIONS /recipe/966 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:31:27] \"GET /recipe/966 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:31:27] \"OPTIONS /search_nearest_image?query=Oyster%20and%20Potato%20Surprise%20Cakes HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:31:27] \"GET /search_nearest_image?query=Oyster%20and%20Potato%20Surprise%20Cakes HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:31:36] \"GET /recipe/109 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:37:46] \"OPTIONS /search_nearest_image?query=poyato HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:37:46] \"GET /search_nearest_image?query=poyato HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:37:46] \"OPTIONS /search?query=poyato HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:37:46] \"GET /search?query=poyato HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:37:50] \"OPTIONS /recipe/665 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:37:50] \"GET /recipe/665 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:37:54] \"OPTIONS /search_nearest_image?query=poyato HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:37:54] \"GET /search_nearest_image?query=poyato HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:37:55] \"OPTIONS /search?query=poyato HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:37:55] \"GET /search?query=poyato HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:37:57] \"OPTIONS /recipe/932 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:37:57] \"GET /recipe/932 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:40:48] \"OPTIONS /search_nearest_image?query=ka HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:40:48] \"GET /search_nearest_image?query=ka HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:40:48] \"OPTIONS /search?query=ka HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:40:49] \"GET /search?query=ka HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:41:34] \"OPTIONS /search_nearest_image?query=kan HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:41:34] \"GET /search_nearest_image?query=kan HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:41:35] \"OPTIONS /search?query=kan HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:41:35] \"GET /search?query=kan HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:41:43] \"OPTIONS /search_nearest_image?query=alas HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:41:43] \"GET /search_nearest_image?query=alas HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:41:44] \"OPTIONS /search?query=alas HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:41:44] \"GET /search?query=alas HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:42:02] \"OPTIONS /search_nearest_image?query=Crock%20Pot%20Lasagna HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:42:02] \"GET /search_nearest_image?query=Crock%20Pot%20Lasagna HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:42:02] \"OPTIONS /search?query=Crock%20Pot%20Lasagna HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:42:03] \"GET /search?query=Crock%20Pot%20Lasagna HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:42:31] \"OPTIONS /recipe/685 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:42:31] \"GET /recipe/685 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:45:32] \"OPTIONS /recipe/685 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:45:32] \"GET /recipe/685 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:50:16] \"OPTIONS /recipe/685 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:50:16] \"GET /recipe/685 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:50:45] \"OPTIONS /search_nearest_image?query=tomat HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:50:45] \"GET /search_nearest_image?query=tomat HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:50:46] \"OPTIONS /search?query=tomat HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:50:46] \"GET /search?query=tomat HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:50:48] \"OPTIONS /recipe/430 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Mar/2025 23:50:48] \"GET /recipe/430 HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "# Run the Flask app on port 5000\n",
    "app.run(port=5000, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Instructions\n",
    "\n",
    "1. **Authentication:** Use a REST client (or cURL) to POST to `/login` with JSON payload, e.g.: \n",
    "   ```json\n",
    "   {\"username\": \"user1\", \"password\": \"password1\"}\n",
    "   ```\n",
    "   You'll receive a token in the response. Use that token in the `Authorization` header for subsequent requests.\n",
    "\n",
    "2. **Search:** GET `/search?query=chicken` with the header `Authorization: <token>` to retrieve matching recipes.\n",
    "\n",
    "3. **Detailed View:** GET `/recipe/<recipe_id>` to fetch full details for a recipe.\n",
    "\n",
    "4. **Bookmarking:** POST to `/bookmark` with JSON payload containing a `recipe_id` and an optional `rating`.\n",
    "\n",
    "5. **Folder Management:** GET or POST to `/folders` to list or create folders.\n",
    "\n",
    "6. **Recommendations:** GET `/recommendations` to retrieve a list of recommended recipes (dummy implementation)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SE-IR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Food Recipe Browser Project\n",
    "\n",
    "By `652115013 Narongchai Rongthong`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we load the data from parquet file provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipes data already loaded.\n",
      "Loaded 522517 recipes.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Check if recipes_df is already loaded\n",
    "if 'recipes_df' not in globals():\n",
    "    recipes_df = pd.read_parquet('resource/recipes.parquet')\n",
    "    recipes_df['RecipeServings'].fillna(0.0, inplace=True)  # Fill NaN with default value\n",
    "    print(f\"Loaded {len(recipes_df)} recipes.\")\n",
    "else:\n",
    "    print(\"Recipes data already loaded.\")\n",
    "    print(f\"Loaded {len(recipes_df)} recipes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Elasticsearch\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es_client = Elasticsearch(\n",
    "    \"https://localhost:9200\",\n",
    "    basic_auth=(\"elastic\", \"_Z9BSk2zcMuFD=-1LlAX\"),\n",
    "    ca_certs=\"~/http_ca.crt\"\n",
    ")\n",
    "\n",
    "if es_client.ping():\n",
    "    print(\"Connected to Elasticsearch\")\n",
    "else:\n",
    "    print(\"Elasticsearch connection failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can start indexing the data\n",
    "- applying fields we need\n",
    "    - id\n",
    "    - name\n",
    "    - ingredients\n",
    "    - instuctions\n",
    "\n",
    "For searching i want to join those together so its easier to find into `cleaned` \"search text\"\n",
    "\n",
    "along with extra cleaned name\n",
    "\n",
    "Through `stemming` and removing `stopwords`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Setup text cleaner\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Exclude specific stopwords\n",
    "important_stop_words =  {\"with\", \"and\"}\n",
    "custom_stopwords = set(stopwords.words('english')) - important_stop_words  \n",
    "\n",
    "def clean_text(text):\n",
    "    tokens = word_tokenize(text.lower())  \n",
    "    filtered_tokens = [word for word in tokens if word not in custom_stopwords]  \n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]  \n",
    "    return \" \".join(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Additionally, since if we send very short query like `\"t\"` or `\"to\"` we'd get completely empty results\n",
    "instead we can make it try to show up something that matches their `ngrams`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_33848\\375437291.py:9: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
      "  es_client.indices.delete(index=index_name, ignore=[400, 404])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created index: recipes\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 209\u001b[0m\n\u001b[0;32m    207\u001b[0m run_all \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_all \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 209\u001b[0m     \u001b[43mbulk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mes_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerate_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecipes_df\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# full size\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     bulk(es_client, generate_docs(recipes_sample)) \u001b[38;5;66;03m# limited size\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\SE-IR\\Lib\\site-packages\\elasticsearch\\helpers\\actions.py:540\u001b[0m, in \u001b[0;36mbulk\u001b[1;34m(client, actions, stats_only, ignore_status, *args, **kwargs)\u001b[0m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;66;03m# make streaming_bulk yield successful results so we can count them\u001b[39;00m\n\u001b[0;32m    539\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myield_ok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 540\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ok, item \u001b[38;5;129;01min\u001b[39;00m streaming_bulk(\n\u001b[0;32m    541\u001b[0m     client, actions, ignore_status\u001b[38;5;241m=\u001b[39mignore_status, span_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhelpers.bulk\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    542\u001b[0m ):\n\u001b[0;32m    543\u001b[0m     \u001b[38;5;66;03m# go through request-response pairs and detect failures\u001b[39;00m\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ok:\n\u001b[0;32m    545\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stats_only:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\SE-IR\\Lib\\site-packages\\elasticsearch\\helpers\\actions.py:435\u001b[0m, in \u001b[0;36mstreaming_bulk\u001b[1;34m(client, actions, chunk_size, max_chunk_bytes, raise_on_error, expand_action_callback, raise_on_exception, max_retries, initial_backoff, max_backoff, yield_ok, ignore_status, retry_on_status, span_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    428\u001b[0m bulk_data: List[\n\u001b[0;32m    429\u001b[0m     Union[\n\u001b[0;32m    430\u001b[0m         Tuple[_TYPE_BULK_ACTION_HEADER],\n\u001b[0;32m    431\u001b[0m         Tuple[_TYPE_BULK_ACTION_HEADER, _TYPE_BULK_ACTION_BODY],\n\u001b[0;32m    432\u001b[0m     ]\n\u001b[0;32m    433\u001b[0m ]\n\u001b[0;32m    434\u001b[0m bulk_actions: List[\u001b[38;5;28mbytes\u001b[39m]\n\u001b[1;32m--> 435\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bulk_data, bulk_actions \u001b[38;5;129;01min\u001b[39;00m _chunk_actions(\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28mmap\u001b[39m(expand_action_callback, actions),\n\u001b[0;32m    437\u001b[0m     chunk_size,\n\u001b[0;32m    438\u001b[0m     max_chunk_bytes,\n\u001b[0;32m    439\u001b[0m     serializer,\n\u001b[0;32m    440\u001b[0m ):\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_retries \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    442\u001b[0m         to_retry: List[\u001b[38;5;28mbytes\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\SE-IR\\Lib\\site-packages\\elasticsearch\\helpers\\actions.py:234\u001b[0m, in \u001b[0;36m_chunk_actions\u001b[1;34m(actions, chunk_size, max_chunk_bytes, serializer)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;124;03mSplit actions into chunks by number or size, serialize them into strings in\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03mthe process.\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    231\u001b[0m chunker \u001b[38;5;241m=\u001b[39m _ActionChunker(\n\u001b[0;32m    232\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39mchunk_size, max_chunk_bytes\u001b[38;5;241m=\u001b[39mmax_chunk_bytes, serializer\u001b[38;5;241m=\u001b[39mserializer\n\u001b[0;32m    233\u001b[0m )\n\u001b[1;32m--> 234\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action, data \u001b[38;5;129;01min\u001b[39;00m actions:\n\u001b[0;32m    235\u001b[0m     ret \u001b[38;5;241m=\u001b[39m chunker\u001b[38;5;241m.\u001b[39mfeed(action, data)\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ret:\n",
      "Cell \u001b[1;32mIn[83], line 160\u001b[0m, in \u001b[0;36mgenerate_docs\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;66;03m# Combine and clean everything\u001b[39;00m\n\u001b[0;32m    159\u001b[0m combined_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([name, description, ingredients_text, instructions_text, keywords_text])\n\u001b[1;32m--> 160\u001b[0m search_text \u001b[38;5;241m=\u001b[39m \u001b[43mclean_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;66;03m# Process image_urls safely\u001b[39;00m\n\u001b[0;32m    163\u001b[0m image_urls_val \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImages\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[72], line 19\u001b[0m, in \u001b[0;36mclean_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     17\u001b[0m tokens \u001b[38;5;241m=\u001b[39m word_tokenize(text\u001b[38;5;241m.\u001b[39mlower())  \n\u001b[0;32m     18\u001b[0m filtered_tokens \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_stopwords]  \n\u001b[1;32m---> 19\u001b[0m stemmed_tokens \u001b[38;5;241m=\u001b[39m [\u001b[43mstemmer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m filtered_tokens]  \n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(stemmed_tokens)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\SE-IR\\Lib\\site-packages\\nltk\\stem\\porter.py:672\u001b[0m, in \u001b[0;36mPorterStemmer.stem\u001b[1;34m(self, word, to_lowercase)\u001b[0m\n\u001b[0;32m    669\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stem\n\u001b[0;32m    671\u001b[0m stem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step1a(stem)\n\u001b[1;32m--> 672\u001b[0m stem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step1b\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    673\u001b[0m stem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step1c(stem)\n\u001b[0;32m    674\u001b[0m stem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step2(stem)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\SE-IR\\Lib\\site-packages\\nltk\\stem\\porter.py:346\u001b[0m, in \u001b[0;36mPorterStemmer._step1b\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replace_suffix(word, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mied\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# (m>0) EED -> EE\u001b[39;00m\n\u001b[1;32m--> 346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mword\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendswith\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    347\u001b[0m     stem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replace_suffix(word, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_measure(stem) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from elasticsearch.helpers import bulk\n",
    "import numpy as np\n",
    "\n",
    "# Define index name and sample size for development\n",
    "index_name = \"recipes\"\n",
    "sample_size = 1000 # Set the sample size for testing (adjust as needed)\n",
    "\n",
    "# Delete the index if it already exists\n",
    "es_client.indices.delete(index=index_name, ignore=[400, 404])\n",
    "\n",
    "# Create the index with a mapping that uses an English analyzer\n",
    "mapping = {\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"tokenizer\": {\n",
    "                \"ngram_tokenizer\": {\n",
    "                    \"type\": \"ngram\",\n",
    "                    \"min_gram\": 2,  # Minimum length of n-grams\n",
    "                    \"max_gram\": 3,  # Maximum length of n-grams\n",
    "                    \"token_chars\": [\"letter\", \"digit\"]\n",
    "                }\n",
    "            },\n",
    "            \"analyzer\": {\n",
    "                \"default\": {\n",
    "                    \"type\": \"english\"\n",
    "                },\n",
    "                \"ngram_analyzer\": {  # Add a custom n-gram analyzer\n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"ngram_tokenizer\",\n",
    "                    \"filter\": [\"lowercase\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"recipe_id\": {\"type\": \"keyword\"},\n",
    "            \"name\": { \n",
    "                \"type\": \"text\", \n",
    "                \"analyzer\": \"english\",\n",
    "                \"fields\": { \n",
    "                    \"ngram\": {  # Add an n-gram variant of the name field\n",
    "                        \"type\": \"text\", \n",
    "                        \"analyzer\": \"ngram_analyzer\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"cleaned_name\": {\"type\": \"text\", \"analyzer\": \"english\"},\n",
    "            \"author_name\": {\"type\": \"text\", \"analyzer\": \"english\"},\n",
    "            \"recipe_category\": {\"type\": \"text\", \"analyzer\": \"english\"},\n",
    "            \"description\": {\"type\": \"text\", \"analyzer\": \"english\"},\n",
    "            \"ingredients\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"english\",\n",
    "                \"fields\": {\n",
    "                    \"raw\": {\n",
    "                        \"type\": \"keyword\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"instructions\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"english\",\n",
    "                \"fields\": {\n",
    "                    \"raw\": {\n",
    "                        \"type\": \"keyword\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"keywords\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"english\",\n",
    "                \"fields\": {\n",
    "                    \"raw\": {\n",
    "                        \"type\": \"keyword\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"search_text\": {\"type\": \"text\", \"analyzer\": \"english\"},\n",
    "            \"image_urls\": {\"type\": \"keyword\"},\n",
    "            # Time-related fields\n",
    "            \"cook_time\": {\"type\": \"text\"},\n",
    "            \"prep_time\": {\"type\": \"text\"},\n",
    "            \"total_time\": {\"type\": \"text\"},\n",
    "            # Nutritional content fields\n",
    "            \"calories\": {\"type\": \"float\"},\n",
    "            \"fat_content\": {\"type\": \"float\"},\n",
    "            \"cholesterol_content\": {\"type\": \"float\"},\n",
    "            \"carbohydrate_content\": {\"type\": \"float\"},\n",
    "            \"fiber_content\": {\"type\": \"float\"},\n",
    "            \"sugar_content\": {\"type\": \"float\"},\n",
    "            \"protein_content\": {\"type\": \"float\"},\n",
    "            \"recipe_servings\": {\"type\": \"float\"},\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# Create the index\n",
    "es_client.indices.create(index=index_name, body=mapping)\n",
    "print(f\"Created index: {index_name}\")\n",
    "\n",
    "# Get a sample of the recipes for development (you can adjust sample size)\n",
    "recipes_sample = recipes_df.head(sample_size)\n",
    "\n",
    "# Prepare the documents for bulk indexing\n",
    "def generate_docs(df):\n",
    "    for idx, row in df.iterrows():\n",
    "        # Main Informations\n",
    "        recipe_id = str(int(float(row.get('RecipeId', idx))))  # Ensures it's always an integer string\n",
    "        name = str(row.get('Name') or '')\n",
    "        cleaned_name = clean_text(name)\n",
    "        author_name = str(row.get('AuthorName') or '')\n",
    "        recipe_category = str(row.get('RecipeCategory') or '')\n",
    "        description = str(row.get('Description') or '')\n",
    "        \n",
    "        # Process ingredients as a list of (ingredient, quantity) pairs\n",
    "        ingredients = list(zip(row.get('RecipeIngredientParts', []), row.get('RecipeIngredientQuantities', [])))\n",
    "        \n",
    "        # Process instructions safely\n",
    "        instructions_val = row.get('RecipeInstructions')\n",
    "        if instructions_val is None:\n",
    "            instructions_list = []\n",
    "        elif isinstance(instructions_val, np.ndarray):\n",
    "            instructions_list = instructions_val.tolist()\n",
    "        else:\n",
    "            instructions_list = instructions_val\n",
    "        instructions_text = \" \".join(map(str, instructions_list)) if instructions_list else ''\n",
    "        \n",
    "        # Process keywords safely\n",
    "        keywords_val = row.get('Keywords')\n",
    "        if keywords_val is None:\n",
    "            keywords = []\n",
    "        elif isinstance(keywords_val, np.ndarray):\n",
    "            keywords = keywords_val.tolist()\n",
    "        else:\n",
    "            keywords = keywords_val\n",
    "        keywords_text = \" \".join(filter(None, map(str, keywords))) if len(keywords) > 0 else ''\n",
    "        \n",
    "        # Time\n",
    "        cook_time = str(row.get('CookTime') or '')\n",
    "        prep_time = str(row.get('PrepTime') or '')\n",
    "        total_time = str(row.get('TotalTime') or '')\n",
    "        \n",
    "        # Nutritional Contents\n",
    "        calories = float(row.get('Calories') or 0.0)\n",
    "        fat_content = float(row.get('FatContent') or 0.0)\n",
    "        cholesterol_content = float(row.get('CholesterolContent') or 0.0)\n",
    "        carbohydrate_content = float(row.get('CarbohydrateContent') or 0.0)\n",
    "        fiber_content = float(row.get('FiberContent') or 0.0)\n",
    "        sugar_content = float(row.get('SugarContent') or 0.0)\n",
    "        protein_content = float(row.get('ProteinContent') or 0.0)\n",
    "        recipe_servings = float(row.get('RecipeServings') or 0.0)\n",
    "        \n",
    "        # Process ingredients into text for search_text\n",
    "        ingredients_text = \" \".join([f\"{str(ing)} {str(qty)}\" for ing, qty in ingredients]) if ingredients else ''\n",
    "        \n",
    "        # Combine and clean everything\n",
    "        combined_text = \" \".join([name, description, ingredients_text, instructions_text, keywords_text])\n",
    "        search_text = clean_text(combined_text)\n",
    "        \n",
    "        # Process image_urls safely\n",
    "        image_urls_val = row.get('Images')\n",
    "        if image_urls_val is None:\n",
    "            image_urls = []\n",
    "        elif isinstance(image_urls_val, np.ndarray):\n",
    "            image_urls = image_urls_val.tolist()\n",
    "        else:\n",
    "            image_urls = image_urls_val\n",
    "\n",
    "        doc = {\n",
    "            \"_op_type\": \"index\",\n",
    "            \"_index\": index_name,\n",
    "            \"_id\": recipe_id,\n",
    "            \"_source\": {\n",
    "                \"recipe_id\": recipe_id,\n",
    "                \"name\": name,\n",
    "                \"cleaned_name\": cleaned_name,\n",
    "                \"author_name\": author_name,\n",
    "                \"recipe_category\": recipe_category,\n",
    "                \"description\": description,\n",
    "                \"ingredients\": ingredients,  # stored as list of tuples\n",
    "                \"instructions\": instructions_list,  # stored as a list\n",
    "                # Time\n",
    "                \"cook_time\": cook_time,\n",
    "                \"prep_time\": prep_time,\n",
    "                \"total_time\": total_time,\n",
    "                # Nutritional Contents\n",
    "                \"calories\": calories,\n",
    "                \"fat_content\": fat_content,\n",
    "                \"cholesterol_content\": cholesterol_content,\n",
    "                \"carbohydrate_content\": carbohydrate_content,\n",
    "                \"fiber_content\": fiber_content,\n",
    "                \"sugar_content\": sugar_content,\n",
    "                \"protein_content\": protein_content,\n",
    "                \"recipe_servings\": recipe_servings,\n",
    "                # Searching words\n",
    "                \"keywords\": keywords,\n",
    "                \"search_text\": search_text,\n",
    "                \"image_urls\": image_urls\n",
    "            }\n",
    "        }\n",
    "        yield doc\n",
    "\n",
    "\n",
    "# Bulk index the sample documents\n",
    "run_all = True\n",
    "if run_all == True:\n",
    "    bulk(es_client, generate_docs(recipes_df)) # full size\n",
    "else:\n",
    "    bulk(es_client, generate_docs(recipes_sample)) # limited size\n",
    "\n",
    "print(f\"Indexed {len(recipes_sample)} recipes into Elasticsearch.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User related database were created in docker-compose's sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create flask app to expose api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Flask API Endpoints ---\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from flask_cors import CORS\n",
    "from flask import Flask, request, jsonify, g\n",
    "from flask_sqlalchemy import SQLAlchemy\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app, supports_credentials=True, resources={r\"/*\": {\"origins\": \"*\"}})\n",
    "# Connection to database\n",
    "app.config['SQLALCHEMY_DATABASE_URI'] = 'mysql+pymysql://user:user_password@localhost:3309/my_database'\n",
    "app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False\n",
    "\n",
    "db = SQLAlchemy(app)\n",
    "# Development mode token (for easier development)\n",
    "DEV_TOKEN = \"dev\" \n",
    "\n",
    "def generate_token():\n",
    "    return str(random.randint(100000, 999999))\n",
    "\n",
    "class User(db.Model):\n",
    "    __tablename__ = \"users\"\n",
    "    \n",
    "    username = db.Column(db.String(50), primary_key=True)\n",
    "    password_hash = db.Column(db.String(255), nullable=False)\n",
    "\n",
    "    sessions = db.relationship(\"Session\", backref=\"user\", cascade=\"all, delete\", lazy=True)\n",
    "    bookmarks = db.relationship(\"Bookmark\", backref=\"user\", cascade=\"all, delete\", lazy=True)\n",
    "    folders = db.relationship(\"Folder\", backref=\"user\", cascade=\"all, delete\", lazy=True)\n",
    "\n",
    "\n",
    "class Session(db.Model):\n",
    "    __tablename__ = \"sessions\"\n",
    "\n",
    "    token = db.Column(db.String(36), primary_key=True, default=lambda: str(uuid.uuid4()))\n",
    "    username = db.Column(db.String(50), db.ForeignKey(\"users.username\", ondelete=\"CASCADE\"), nullable=False)\n",
    "\n",
    "\n",
    "class Bookmark(db.Model):\n",
    "    __tablename__ = \"bookmarks\"\n",
    "\n",
    "    id = db.Column(db.Integer, primary_key=True, autoincrement=True)\n",
    "    username = db.Column(db.String(50), db.ForeignKey(\"users.username\", ondelete=\"CASCADE\"), nullable=False)\n",
    "    recipe_id = db.Column(db.Integer, nullable=False)\n",
    "    rating = db.Column(db.Integer, nullable=True)\n",
    "\n",
    "    created_at = db.Column(db.TIMESTAMP, server_default=db.func.current_timestamp())\n",
    "\n",
    "    __table_args__ = (\n",
    "        db.CheckConstraint(\"rating BETWEEN 1 AND 5\", name=\"valid_rating\"),\n",
    "    )\n",
    "\n",
    "\n",
    "class Folder(db.Model):\n",
    "    __tablename__ = \"folders\"\n",
    "\n",
    "    id = db.Column(db.Integer, primary_key=True, autoincrement=True)\n",
    "    username = db.Column(db.String(50), db.ForeignKey(\"users.username\", ondelete=\"CASCADE\"), nullable=False)\n",
    "    folder_name = db.Column(db.String(100), nullable=False)\n",
    "\n",
    "    folder_recipes = db.relationship(\"FolderRecipe\", backref=\"folder\", cascade=\"all, delete\", lazy=True)\n",
    "\n",
    "    __table_args__ = (\n",
    "        db.UniqueConstraint(\"username\", \"folder_name\", name=\"unique_folder\"),\n",
    "    )\n",
    "\n",
    "\n",
    "class FolderRecipe(db.Model):\n",
    "    __tablename__ = \"folder_recipes\"\n",
    "\n",
    "    folder_id = db.Column(db.Integer, db.ForeignKey(\"folders.id\", ondelete=\"CASCADE\"), primary_key=True)\n",
    "    recipe_id = db.Column(db.Integer, primary_key=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# App routes\n",
    "\n",
    "@app.before_request\n",
    "def start_timer():\n",
    "    g.start_time = time.time()\n",
    "\n",
    "@app.after_request\n",
    "def add_elapsed_time(response):\n",
    "    if hasattr(g, 'start_time'):\n",
    "        response_time = time.time() - g.start_time\n",
    "        response_json = response.get_json()\n",
    "        if response_json:  # Only modify if response is JSON\n",
    "            response_json[\"response_time\"] = round(response_time, 4)\n",
    "            response.set_data(json.dumps(response_json))  # Update response body\n",
    "    return response\n",
    "\n",
    "# USER HANDLING\n",
    "# UC-001: User Authentication (using the database)\n",
    "@app.route('/login', methods=['POST'])\n",
    "def login():\n",
    "    data = request.get_json()\n",
    "    username = data.get(\"username\")\n",
    "    password = data.get(\"password\")\n",
    "    \n",
    "    user = User.query.filter_by(username=username).first()\n",
    "    if user and user.password_hash == password:\n",
    "        token = generate_token()\n",
    "        new_session = Session(token=token, user_id=user.id)\n",
    "        db.session.add(new_session)\n",
    "        db.session.commit()\n",
    "        return jsonify({\"message\": \"Login successful\", \"token\": token})\n",
    "    \n",
    "    return jsonify({\"message\": \"Invalid credentials\"}), 401\n",
    "\n",
    "@app.route('/logout', methods=['POST'])\n",
    "def logout():\n",
    "    token = request.headers.get(\"Authorization\")\n",
    "    session_obj = Session.query.filter_by(token=token).first()\n",
    "    if session_obj:\n",
    "        db.session.delete(session_obj)\n",
    "        db.session.commit()\n",
    "        return jsonify({\"message\": \"Logout successful\"})\n",
    "    \n",
    "    return jsonify({\"message\": \"Invalid token\"}), 401\n",
    "\n",
    "# Helper function to check authentication\n",
    "def is_authenticated(request):\n",
    "    token = request.headers.get(\"Authorization\")\n",
    "    if token == DEV_TOKEN:\n",
    "        return True\n",
    "    return Session.query.filter_by(token=token).first() is not None\n",
    "\n",
    "# SEARCHING\n",
    "# UC-002 & UC-003: Recipe Search Functionality & Display Results\n",
    "@app.route('/search', methods=['GET'])\n",
    "def search():\n",
    "    if not is_authenticated(request):\n",
    "        return jsonify({\"message\": \"Unauthorized\"}), 401\n",
    "    \n",
    "    query = request.args.get(\"query\", \"\")\n",
    "    cleaned_query = clean_text(query)\n",
    "    res = es_client.search(index=index_name, body={\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"should\": [\n",
    "                    { \"match\": { \"name\": { \"query\": query, \"boost\": 3 } } },\n",
    "                    { \"match\": { \"name.ngram\": { \"query\": query, \"boost\": 2 } } },\n",
    "                    { \"match\": { \"stemmed_name\": { \"query\": cleaned_query, \"boost\": 2 } } },\n",
    "                    { \"match\": { \"search_text\": { \"query\": cleaned_query, \"fuzziness\": \"AUTO\", \"boost\": 1 } } }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    })\n",
    "    hits = res[\"hits\"][\"hits\"]\n",
    "    results = [\n",
    "        {\n",
    "            \"recipe_id\": hit[\"_source\"][\"recipe_id\"],\n",
    "            \"name\": hit[\"_source\"][\"name\"],\n",
    "            \"snippet\": hit[\"_source\"][\"description\"][:75],\n",
    "            \"image_urls\": hit[\"_source\"].get(\"image_urls\", \"\")\n",
    "        } for hit in hits\n",
    "    ]\n",
    "    return jsonify({\"results\": results})\n",
    "\n",
    "@app.route('/search_nearest_image', methods=['GET'])\n",
    "def search_nearest_image():\n",
    "    if not is_authenticated(request):\n",
    "        return jsonify({\"message\": \"Unauthorized\"}), 401\n",
    "    \n",
    "    query = request.args.get(\"query\", \"\")\n",
    "    cleaned_query = clean_text(query)\n",
    "    res = es_client.search(index=index_name, body={\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"should\": [\n",
    "                    { \"match\": { \"name\": { \"query\": query, \"boost\": 3 } } },\n",
    "                    { \"match\": { \"name.ngram\": { \"query\": query, \"boost\": 2 } } },\n",
    "                    { \"match\": { \"stemmed_name\": { \"query\": cleaned_query, \"boost\": 2 } } },\n",
    "                    { \"match\": { \"search_text\": { \"query\": cleaned_query, \"fuzziness\": \"AUTO\", \"boost\": 1 } } }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    hits = res[\"hits\"][\"hits\"]\n",
    "    for hit in hits:\n",
    "        top_hit = hit[\"_source\"]\n",
    "        if \"image_urls\" in top_hit and top_hit[\"image_urls\"]:\n",
    "            return jsonify({\"result\": {\n",
    "                \"recipe_id\": top_hit[\"recipe_id\"],\n",
    "                \"name\": top_hit[\"name\"],\n",
    "                \"image_urls\": top_hit[\"image_urls\"]\n",
    "            }})\n",
    "    \n",
    "    return jsonify({\"message\": \"No results with images found\"}), 404\n",
    "\n",
    "# UC-004: Detailed Dish Information\n",
    "@app.route('/recipe/<recipe_id>', methods=['GET'])\n",
    "def recipe_detail(recipe_id):\n",
    "    if not is_authenticated(request):\n",
    "        return jsonify({\"message\": \"Unauthorized\"}), 401\n",
    "    res = es_client.get(index=index_name, id=recipe_id)\n",
    "    result = res[\"_source\"]\n",
    "    result.pop(\"cleaned_name\", None)\n",
    "    result.pop(\"search_text\", None)\n",
    "    return jsonify(result)\n",
    "\n",
    "# UC-006: Bookmarking and Rating (using the database)\n",
    "@app.route('/bookmark', methods=['POST'])\n",
    "def bookmark():\n",
    "    if not is_authenticated(request):\n",
    "        return jsonify({\"message\": \"Unauthorized\"}), 401\n",
    "    \n",
    "    data = request.get_json()\n",
    "    recipe_id = data.get(\"recipe_id\")\n",
    "    rating = data.get(\"rating\")\n",
    "    token = request.headers.get(\"Authorization\")\n",
    "    session_obj = Session.query.filter_by(token=token).first()\n",
    "    if not session_obj:\n",
    "        return jsonify({\"message\": \"Invalid session\"}), 401\n",
    "    \n",
    "    new_bookmark = Bookmark(user_id=session_obj.user_id, recipe_id=recipe_id, rating=rating)\n",
    "    db.session.add(new_bookmark)\n",
    "    db.session.commit()\n",
    "    return jsonify({\"message\": \"Bookmarked successfully\"})\n",
    "\n",
    "# UC-005: Folder Management\n",
    "@app.route('/folders', methods=['GET', 'POST'])\n",
    "def folders():\n",
    "    if not is_authenticated(request):\n",
    "        return jsonify({\"message\": \"Unauthorized\"}), 401\n",
    "    username = sessions.get(request.headers.get(\"Authorization\"), \"dev_user\")\n",
    "    if request.method == 'GET':\n",
    "        return jsonify(user_folders.get(username, {}))\n",
    "    elif request.method == 'POST':\n",
    "        data = request.get_json()\n",
    "        folder_name = data.get(\"folder_name\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "# Run the Flask app on port 5000\n",
    "app.run(port=5000, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Instructions\n",
    "\n",
    "1. **Authentication:** Use a REST client (or cURL) to POST to `/login` with JSON payload, e.g.: \n",
    "   ```json\n",
    "   {\"username\": \"user1\", \"password\": \"password1\"}\n",
    "   ```\n",
    "   You'll receive a token in the response. Use that token in the `Authorization` header for subsequent requests.\n",
    "\n",
    "2. **Search:** GET `/search?query=chicken` with the header `Authorization: <token>` to retrieve matching recipes.\n",
    "\n",
    "3. **Detailed View:** GET `/recipe/<recipe_id>` to fetch full details for a recipe.\n",
    "\n",
    "4. **Bookmarking:** POST to `/bookmark` with JSON payload containing a `recipe_id` and an optional `rating`.\n",
    "\n",
    "5. **Folder Management:** GET or POST to `/folders` to list or create folders.\n",
    "\n",
    "6. **Recommendations:** GET `/recommendations` to retrieve a list of recommended recipes (dummy implementation)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SE-IR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

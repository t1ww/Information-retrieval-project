{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Food Recipe Browser Project\n",
    "\n",
    "By `652115013 Narongchai Rongthong`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we load the data from parquet file provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipes data already loaded.\n",
      "Loaded 522517 recipes.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Check if recipes_df is already loaded\n",
    "if 'recipes_df' not in globals():\n",
    "    recipes_df = pd.read_parquet('../resource/recipes.parquet')\n",
    "    recipes_df['RecipeServings'].fillna(0.0, inplace=True)  # Fill NaN with default value\n",
    "    print(f\"Loaded {len(recipes_df)} recipes.\")\n",
    "else:\n",
    "    print(\"Recipes data already loaded.\")\n",
    "    print(f\"Loaded {len(recipes_df)} recipes.\") # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "Elasticsearch connection failed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnected to Elasticsearch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionRefusedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mElasticsearch connection failed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: Elasticsearch connection failed"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Get values from .env\n",
    "es_password = os.getenv(\"ES_PASSWORD\")\n",
    "\n",
    "index_name = \"recipes\"\n",
    "\n",
    "# Create Elasticsearch client\n",
    "es_client = Elasticsearch(\n",
    "    \"https://localhost:9200\",\n",
    "    basic_auth=(\"elastic\", es_password),\n",
    "    ca_certs='~/http_ca.crt'\n",
    ")\n",
    "\n",
    "if es_client.ping():\n",
    "    print(\"Connected to Elasticsearch\")\n",
    "else:\n",
    "    raise ConnectionRefusedError(\"Elasticsearch connection failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can start indexing the data\n",
    "- applying fields we need\n",
    "    - id\n",
    "    - name\n",
    "    - ingredients\n",
    "    - instuctions\n",
    "\n",
    "For searching i want to join those together so its easier to find into `cleaned` \"search text\"\n",
    "\n",
    "along with extra cleaned name\n",
    "\n",
    "Through `stemming` and removing `stopwords`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Setup text cleaner\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Exclude specific stopwords\n",
    "important_stop_words =  {\"with\", \"and\"}\n",
    "custom_stopwords = set(stopwords.words('english')) - important_stop_words  \n",
    "\n",
    "def clean_text(text):\n",
    "    tokens = word_tokenize(text.lower())  \n",
    "    filtered_tokens = [word for word in tokens if word not in custom_stopwords]  \n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]  \n",
    "    return \" \".join(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Additionally, since if we send very short query like `\"t\"` or `\"to\"` we'd get completely empty results\n",
    "instead we can make it try to show up something that matches their `ngrams`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_15752\\2739054999.py:8: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
      "  es_client.indices.delete(index=index_name, ignore=[400, 404])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created index: recipes\n",
      "Indexed 20000 recipes into Elasticsearch.\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch.helpers import bulk\n",
    "import numpy as np\n",
    "\n",
    "# Define index name and sample size for development\n",
    "sample_size = 20000 # Set the sample size for testing (adjust as needed)\n",
    "\n",
    "# Delete the index if it already exists\n",
    "es_client.indices.delete(index=index_name, ignore=[400, 404])\n",
    "\n",
    "# Create the index with a mapping that uses an English analyzer\n",
    "mapping = {\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"tokenizer\": {\n",
    "                \"ngram_tokenizer\": {\n",
    "                    \"type\": \"ngram\",\n",
    "                    \"min_gram\": 2,  # Minimum length of n-grams\n",
    "                    \"max_gram\": 3,  # Maximum length of n-grams\n",
    "                    \"token_chars\": [\"letter\", \"digit\"]\n",
    "                }\n",
    "            },\n",
    "            \"analyzer\": {\n",
    "                \"default\": {\n",
    "                    \"type\": \"english\"\n",
    "                },\n",
    "                \"ngram_analyzer\": {  # Add a custom n-gram analyzer\n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"ngram_tokenizer\",\n",
    "                    \"filter\": [\"lowercase\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"recipe_id\": {\"type\": \"keyword\"},\n",
    "            \"name\": { \n",
    "                \"type\": \"text\", \n",
    "                \"analyzer\": \"english\",\n",
    "                \"fields\": { \n",
    "                    \"ngram\": {  # Add an n-gram variant of the name field\n",
    "                        \"type\": \"text\", \n",
    "                        \"analyzer\": \"ngram_analyzer\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"cleaned_name\": {\"type\": \"text\", \"analyzer\": \"english\"},\n",
    "            \"author_name\": {\"type\": \"text\", \"analyzer\": \"english\"},\n",
    "            \"recipe_category\": {\"type\": \"text\", \"analyzer\": \"english\"},\n",
    "            \"description\": {\"type\": \"text\", \"analyzer\": \"english\"},\n",
    "            \"ingredients\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"english\",\n",
    "                \"fields\": {\n",
    "                    \"raw\": {\n",
    "                        \"type\": \"keyword\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"instructions\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"english\",\n",
    "                \"fields\": {\n",
    "                    \"raw\": {\n",
    "                        \"type\": \"keyword\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"keywords\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"english\",\n",
    "                \"fields\": {\n",
    "                    \"raw\": {\n",
    "                        \"type\": \"keyword\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"search_text\": {\"type\": \"text\", \"analyzer\": \"english\"},\n",
    "            \"image_urls\": {\"type\": \"keyword\"},\n",
    "            # Time-related fields\n",
    "            \"cook_time\": {\"type\": \"text\"},\n",
    "            \"prep_time\": {\"type\": \"text\"},\n",
    "            \"total_time\": {\"type\": \"text\"},\n",
    "            # Nutritional content fields\n",
    "            \"calories\": {\"type\": \"float\"},\n",
    "            \"fat_content\": {\"type\": \"float\"},\n",
    "            \"cholesterol_content\": {\"type\": \"float\"},\n",
    "            \"carbohydrate_content\": {\"type\": \"float\"},\n",
    "            \"fiber_content\": {\"type\": \"float\"},\n",
    "            \"sugar_content\": {\"type\": \"float\"},\n",
    "            \"protein_content\": {\"type\": \"float\"},\n",
    "            \"recipe_servings\": {\"type\": \"float\"},\n",
    "            # Allergens\n",
    "            \"allergens\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"english\",\n",
    "                \"fields\": {\n",
    "                    \"raw\": {\n",
    "                        \"type\": \"keyword\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create the index\n",
    "es_client.indices.create(index=index_name, body=mapping)\n",
    "print(f\"Created index: {index_name}\")\n",
    "\n",
    "# Get a sample of the recipes for development (you can adjust sample size)\n",
    "recipes_sample = recipes_df.head(sample_size)\n",
    "\n",
    "# Detects allergens helper\n",
    "ALLERGENS = {\n",
    "    \"peanuts\": \"peanuts\",\n",
    "    \"milk\": \"dairy\",\n",
    "    \"cheese\": \"dairy\",\n",
    "    \"butter\": \"dairy\",\n",
    "    \"wheat\": \"gluten\",\n",
    "    \"flour\": \"gluten\",\n",
    "    \"soy sauce\": \"soy\",\n",
    "    \"soybean\": \"soy\",\n",
    "    \"shrimp\": \"shellfish\",\n",
    "    \"crab\": \"shellfish\",\n",
    "    \"lobster\": \"shellfish\",\n",
    "    \"egg\": \"egg\",\n",
    "    \"almond\": \"tree nuts\",\n",
    "    \"cashew\": \"tree nuts\",\n",
    "    \"walnut\": \"tree nuts\",\n",
    "}\n",
    "def detect_allergens(ingredients):\n",
    "    detected_allergens = set()\n",
    "    for ingredient, _ in ingredients:  # Extract ingredient names\n",
    "        for allergen, category in ALLERGENS.items():\n",
    "            if allergen in ingredient.lower():  # Check for allergen keywords\n",
    "                detected_allergens.add(category)\n",
    "    return list(detected_allergens)\n",
    "\n",
    "# Prepare the documents for bulk indexing\n",
    "def generate_docs(df):\n",
    "    for idx, row in df.iterrows():\n",
    "        # Main Informations\n",
    "        recipe_id = str(int(float(row.get('RecipeId', idx))))  # Ensures it's always an integer string\n",
    "        name = str(row.get('Name') or '')\n",
    "        cleaned_name = clean_text(name)\n",
    "        author_name = str(row.get('AuthorName') or '')\n",
    "        recipe_category = str(row.get('RecipeCategory') or '')\n",
    "        description = str(row.get('Description') or '')\n",
    "        \n",
    "        # Process ingredients as a list of (ingredient, quantity) pairs\n",
    "        ingredients = list(zip(row.get('RecipeIngredientParts', []), row.get('RecipeIngredientQuantities', [])))\n",
    "        \n",
    "        # Process instructions safely\n",
    "        instructions_val = row.get('RecipeInstructions')\n",
    "        if instructions_val is None:\n",
    "            instructions_list = []\n",
    "        elif isinstance(instructions_val, np.ndarray):\n",
    "            instructions_list = instructions_val.tolist()\n",
    "        else:\n",
    "            instructions_list = instructions_val\n",
    "        instructions_text = \" \".join(map(str, instructions_list)) if instructions_list else ''\n",
    "        \n",
    "        # Process keywords safely\n",
    "        keywords_val = row.get('Keywords')\n",
    "        if keywords_val is None:\n",
    "            keywords = []\n",
    "        elif isinstance(keywords_val, np.ndarray):\n",
    "            keywords = keywords_val.tolist()\n",
    "        else:\n",
    "            keywords = keywords_val\n",
    "        keywords_text = \" \".join(filter(None, map(str, keywords))) if len(keywords) > 0 else ''\n",
    "        \n",
    "        # Time\n",
    "        cook_time = str(row.get('CookTime') or '')\n",
    "        prep_time = str(row.get('PrepTime') or '')\n",
    "        total_time = str(row.get('TotalTime') or '')\n",
    "        \n",
    "        # Nutritional Contents\n",
    "        calories = float(row.get('Calories') or 0.0)\n",
    "        fat_content = float(row.get('FatContent') or 0.0)\n",
    "        cholesterol_content = float(row.get('CholesterolContent') or 0.0)\n",
    "        carbohydrate_content = float(row.get('CarbohydrateContent') or 0.0)\n",
    "        fiber_content = float(row.get('FiberContent') or 0.0)\n",
    "        sugar_content = float(row.get('SugarContent') or 0.0)\n",
    "        protein_content = float(row.get('ProteinContent') or 0.0)\n",
    "        recipe_servings = float(row.get('RecipeServings') or 0.0)\n",
    "        \n",
    "        # Process ingredients into text for search_text\n",
    "        ingredients_text = \" \".join([f\"{str(ing)} {str(qty)}\" for ing, qty in ingredients]) if ingredients else ''\n",
    "        \n",
    "        # Combine and clean everything\n",
    "        combined_text = \" \".join([name, description, ingredients_text, instructions_text, keywords_text])\n",
    "        search_text = clean_text(combined_text)\n",
    "        \n",
    "        # Process image_urls safely\n",
    "        image_urls_val = row.get('Images')\n",
    "        if image_urls_val is None:\n",
    "            image_urls = []\n",
    "        elif isinstance(image_urls_val, np.ndarray):\n",
    "            image_urls = image_urls_val.tolist()\n",
    "        else:\n",
    "            image_urls = image_urls_val\n",
    "            \n",
    "        # Detects allergens\n",
    "        allergens = detect_allergens(ingredients)\n",
    "        \n",
    "        doc = {\n",
    "            \"_op_type\": \"index\",\n",
    "            \"_index\": index_name,\n",
    "            \"_id\": recipe_id,\n",
    "            \"_source\": {\n",
    "                \"recipe_id\": recipe_id,\n",
    "                \"name\": name,\n",
    "                \"cleaned_name\": cleaned_name,\n",
    "                \"author_name\": author_name,\n",
    "                \"recipe_category\": recipe_category,\n",
    "                \"description\": description,\n",
    "                \"ingredients\": ingredients,  # stored as list of tuples\n",
    "                \"instructions\": instructions_list,  # stored as a list\n",
    "                # Time\n",
    "                \"cook_time\": cook_time,\n",
    "                \"prep_time\": prep_time,\n",
    "                \"total_time\": total_time,\n",
    "                # Nutritional Contents\n",
    "                \"calories\": calories,\n",
    "                \"fat_content\": fat_content,\n",
    "                \"cholesterol_content\": cholesterol_content,\n",
    "                \"carbohydrate_content\": carbohydrate_content,\n",
    "                \"fiber_content\": fiber_content,\n",
    "                \"sugar_content\": sugar_content,\n",
    "                \"protein_content\": protein_content,\n",
    "                \"recipe_servings\": recipe_servings,\n",
    "                # Searching words\n",
    "                \"keywords\": keywords,\n",
    "                \"search_text\": search_text,\n",
    "                \"image_urls\": image_urls,\n",
    "                # Allergens\n",
    "                \"allergens\": allergens\n",
    "            }\n",
    "        }\n",
    "        yield doc\n",
    "\n",
    "\n",
    "# Bulk index the sample documents\n",
    "run_all = False\n",
    "if run_all == True:\n",
    "    sample_size = len(recipes_df)\n",
    "    bulk(es_client, generate_docs(recipes_df)) # full size\n",
    "else:\n",
    "    bulk(es_client, generate_docs(recipes_sample)) # limited size\n",
    "\n",
    "print(f\"Indexed {sample_size} recipes into Elasticsearch.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User related database were created in docker-compose's sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can Create flask app to expose api (Moved to api.ipynb for better organization)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SE-IR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
